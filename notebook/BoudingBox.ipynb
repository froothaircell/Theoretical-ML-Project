{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ac64f8-6dd6-44c6-8a12-7ca23a98b189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Set configurations and paths\n",
    "BASE_DIR = \"coco\"  # Base directory for the COCO dataset\n",
    "SPLIT_DIR = os.path.join(BASE_DIR, \"splits\")\n",
    "ANNOTATION_SPLITS_DIR = os.path.join(BASE_DIR, \"annotation_splits\")\n",
    "IMAGE_SIZE = (224, 224)  # Resize all images to 224x224\n",
    "BATCH_SIZE = 128\n",
    "NUM_WORKERS = 4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Mean and std for normalization (ImageNet values)\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "class COCOCustomDataset(Dataset):\n",
    "    def __init__(self, split, transform=None):\n",
    "        \"\"\"\n",
    "        Custom COCO Dataset for bounding box regression.\n",
    "        Args:\n",
    "            split (str): One of 'train', 'val', or 'test'.\n",
    "            transform (callable, optional): Transformations to apply to the images and bounding boxes.\n",
    "        \"\"\"\n",
    "        self.image_dir = os.path.join(SPLIT_DIR, split)\n",
    "        self.annotation_file = os.path.join(ANNOTATION_SPLITS_DIR, f\"instances_{split}2017.json\")\n",
    "        self.transform = transform\n",
    "\n",
    "        # Load annotations\n",
    "        with open(self.annotation_file, \"r\") as f:\n",
    "            self.annotations = json.load(f)\n",
    "\n",
    "        # Map image IDs to file names\n",
    "        self.image_id_to_filename = {img[\"id\"]: img[\"file_name\"] for img in self.annotations[\"images\"]}\n",
    "\n",
    "        # Prepare data\n",
    "        self.data = self._prepare_bounding_boxes()\n",
    "\n",
    "    def _prepare_bounding_boxes(self):\n",
    "        \"\"\"Prepare bounding boxes for regression.\"\"\"\n",
    "        data = []\n",
    "        for ann in self.annotations[\"annotations\"]:\n",
    "            image_id = ann[\"image_id\"]\n",
    "            bbox = ann[\"bbox\"]  # COCO format: [x_min, y_min, width, height]\n",
    "            # Convert bbox to [x1, y1, x2, y2] format\n",
    "            x1 = bbox[0]\n",
    "            y1 = bbox[1]\n",
    "            x2 = bbox[0] + bbox[2]\n",
    "            y2 = bbox[1] + bbox[3]\n",
    "            bbox_converted = [x1, y1, x2, y2]\n",
    "            data.append({\"image_id\": image_id, \"bbox\": bbox_converted})\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_item = self.data[idx]\n",
    "        image_id = data_item[\"image_id\"]\n",
    "        image_file = os.path.join(self.image_dir, self.image_id_to_filename[image_id])\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(image_file).convert(\"RGB\")\n",
    "        width, height = image.size\n",
    "\n",
    "        # Load bbox\n",
    "        bbox = data_item[\"bbox\"]  # [x1, y1, x2, y2]\n",
    "\n",
    "        # Apply transformations (image and bbox)\n",
    "        if self.transform:\n",
    "            image, bbox = self.transform(image, bbox)\n",
    "\n",
    "        return image, bbox\n",
    "\n",
    "class ResizeNormalizeAndToTensor:\n",
    "    def __init__(self, size, mean, std):\n",
    "        \"\"\"\n",
    "        Custom transform to resize image and adjust bounding boxes accordingly,\n",
    "        then convert image to tensor and normalize.\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, image, bbox):\n",
    "        # Resize image\n",
    "        original_width, original_height = image.size\n",
    "        image = image.resize(self.size)\n",
    "\n",
    "        # Scale bbox\n",
    "        scale_x = self.size[0] / original_width\n",
    "        scale_y = self.size[1] / original_height\n",
    "        bbox = [\n",
    "            bbox[0] * scale_x,\n",
    "            bbox[1] * scale_y,\n",
    "            bbox[2] * scale_x,\n",
    "            bbox[3] * scale_y\n",
    "        ]\n",
    "\n",
    "        # Convert image to tensor and normalize\n",
    "        image = transforms.ToTensor()(image)\n",
    "        image = transforms.Normalize(self.mean, self.std)(image)\n",
    "        bbox = torch.tensor(bbox, dtype=torch.float32)\n",
    "\n",
    "        return image, bbox\n",
    "\n",
    "# Define transformations\n",
    "transform = ResizeNormalizeAndToTensor(IMAGE_SIZE, mean, std)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = COCOCustomDataset(split=\"train\", transform=transform)\n",
    "val_dataset = COCOCustomDataset(split=\"val\", transform=transform)\n",
    "test_dataset = COCOCustomDataset(split=\"test\", transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"Validation dataset: {len(val_dataset)} samples\")\n",
    "print(f\"Test dataset: {len(test_dataset)} samples\")\n",
    "\n",
    "def unnormalize_image(image_tensor, mean, std):\n",
    "    \"\"\"\n",
    "    Unnormalize a tensor image for visualization.\n",
    "    \"\"\"\n",
    "    mean = torch.tensor(mean).view(-1, 1, 1)\n",
    "    std = torch.tensor(std).view(-1, 1, 1)\n",
    "    image_tensor = image_tensor * std + mean\n",
    "    return image_tensor\n",
    "\n",
    "def visualize_sample_with_annotation(dataset, index=0):\n",
    "    \"\"\"\n",
    "    Visualize a single image with its ground truth bounding box and print its annotation on the image.\n",
    "    Args:\n",
    "        dataset: The dataset object (e.g., train_dataset, val_dataset, test_dataset).\n",
    "        index: The index of the sample to visualize.\n",
    "    \"\"\"\n",
    "    # Get the image and bbox from the dataset\n",
    "    image, bbox = dataset[index]\n",
    "\n",
    "    # Un-normalize image\n",
    "    image = unnormalize_image(image, mean, std)\n",
    "\n",
    "    # Convert tensor image back to numpy for visualization\n",
    "    image_np = image.permute(1, 2, 0).numpy()  # Convert from CxHxW to HxWxC\n",
    "    image_np = (image_np * 255).astype(\"uint8\")  # Convert to uint8 for visualization\n",
    "\n",
    "    # Plot the image and bbox\n",
    "    fig, ax = plt.subplots(1, figsize=(8, 8))\n",
    "    ax.imshow(image_np)\n",
    "    # Draw the bounding box\n",
    "    rect = patches.Rectangle(\n",
    "        (bbox[0].item(), bbox[1].item()),  # (x_min, y_min)\n",
    "        bbox[2].item() - bbox[0].item(),  # width\n",
    "        bbox[3].item() - bbox[1].item(),  # height\n",
    "        linewidth=2,\n",
    "        edgecolor=\"r\",\n",
    "        facecolor=\"none\",\n",
    "    )\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    # Annotate the image with the bounding box coordinates\n",
    "    bbox_annotation = f\"Bounding Box: {bbox.tolist()}\"\n",
    "    ax.text(\n",
    "        10, 10,  # Position at the top-left corner\n",
    "        bbox_annotation,\n",
    "        color=\"yellow\",\n",
    "        fontsize=12,\n",
    "        bbox=dict(facecolor=\"black\", alpha=0.7),\n",
    "    )\n",
    "\n",
    "    ax.set_title(f\"Ground Truth Bounding Box (Index {index})\")\n",
    "    plt.axis(\"off\")  # Remove axis for better visualization\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "visualize_sample_with_annotation(train_dataset, index=4)\n",
    "\n",
    "def print_input_and_target_shapes(dataloader):\n",
    "    \"\"\"\n",
    "    Print the shapes of inputs and targets in a batch.\n",
    "    Args:\n",
    "        dataloader: DataLoader object.\n",
    "    \"\"\"\n",
    "    for images, targets in dataloader:\n",
    "        print(f\"Input Shape (Images): {images.shape}\")  # Shape of the input images\n",
    "        print(f\"Target Shape (Bounding Boxes): {targets.shape}\")  # Shape of the target bounding boxes\n",
    "        print(f\"Example Target Bounding Box: {targets[0]}\")  # Show an example bounding box\n",
    "        break  # Print for one batch only\n",
    "\n",
    "# Example usage with the train loader\n",
    "print_input_and_target_shapes(train_loader)\n",
    "\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_size=(3, 224, 224), output_size=4):\n",
    "        super(MLPModel, self).__init__()\n",
    "        self.input_dim = input_size[0] * input_size[1] * input_size[2]  # Flatten image dimensions\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.input_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, output_size)  # Output 4 bounding box coordinates\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, output_size=4):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Reduce to 112x112\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # Reduce to 56x56\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1)  # Reduce to 1x1\n",
    "            )\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, output_size)  # Output 4 bounding box coordinates\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        return self.regressor(features)\n",
    "\n",
    "class ViTModel(nn.Module):\n",
    "    def __init__(self, image_size=224, patch_size=16, dim=768, depth=6, heads=8, mlp_dim=1024, output_size=4):\n",
    "        super(ViTModel, self).__init__()\n",
    "        assert image_size % patch_size == 0, \"Image size must be divisible by patch size\"\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        self.patch_dim = 3 * patch_size * patch_size\n",
    "\n",
    "        self.patch_embedding = nn.Linear(self.patch_dim, dim)\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=mlp_dim),\n",
    "            num_layers=depth\n",
    "        )\n",
    "\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, output_size)  # Output 4 bounding box coordinates\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        # Create patches\n",
    "        patches = x.unfold(2, 16, 16).unfold(3, 16, 16)\n",
    "        patches = patches.contiguous().view(batch_size, -1, self.patch_dim)\n",
    "        x = self.patch_embedding(patches)\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.positional_embedding\n",
    "        x = self.transformer(x.permute(1, 0, 2)).permute(1, 0, 2)\n",
    "        return self.regressor(x[:, 0])\n",
    "\n",
    "class TransformerEncoderModel(nn.Module):\n",
    "    def __init__(self, image_size=224, dim=768, depth=6, heads=8, mlp_dim=1024, output_size=4):\n",
    "        super(TransformerEncoderModel, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_embedding = nn.Linear(3 * image_size * image_size, dim)\n",
    "\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=mlp_dim),\n",
    "            num_layers=depth\n",
    "        )\n",
    "\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, output_size)  # Output 4 bounding box coordinates\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear_embedding(x)\n",
    "        x = self.transformer(x.unsqueeze(1)).squeeze(1)\n",
    "        return self.regressor(x)\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"./regressors\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def calculate_regression_metrics(preds, targets):\n",
    "    \"\"\"Calculate regression metrics: MAE, MSE, and R2.\"\"\"\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "    targets = targets.detach().cpu().numpy()\n",
    "\n",
    "    mae = abs(preds - targets).mean()\n",
    "    mse = ((preds - targets) ** 2).mean()\n",
    "    r2 = r2_score(targets, preds)\n",
    "\n",
    "    return mae, mse, r2\n",
    "\n",
    "def train_model(model, train_loader, val_loader, model_name, num_epochs=10, learning_rate=1e-4):\n",
    "    \"\"\"Train and validate a single model.\"\"\"\n",
    "    print(f\"Training {model_name}...\")\n",
    "    model.to(device)\n",
    "    criterion = nn.SmoothL1Loss()  # Smooth L1 Loss for bounding box regression\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for images, targets in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} (Train)\"):\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            preds = model(images)\n",
    "            loss = criterion(preds, targets)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        maes = []\n",
    "        mses = []\n",
    "        with torch.no_grad():\n",
    "            for images, targets in tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} (Validation)\"):\n",
    "                images, targets = images.to(device), targets.to(device)\n",
    "\n",
    "                preds = model(images)\n",
    "                loss = criterion(preds, targets)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Metrics\n",
    "                mae, mse, _ = calculate_regression_metrics(preds, targets)\n",
    "                maes.append(mae)\n",
    "                mses.append(mse)\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        mean_mae = sum(maes) / len(maes)\n",
    "        mean_mse = sum(mses) / len(mses)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, MAE: {mean_mae:.4f}, MSE: {mean_mse:.4f}\")\n",
    "\n",
    "        # Save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), os.path.join(output_dir, f\"{model_name}.pth\"))\n",
    "            print(f\"Best model saved for {model_name}!\")\n",
    "\n",
    "# Adjusted loaders with appropriate batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "# Models\n",
    "#mlp_model = MLPModel(input_size=(3, 224, 224), output_size=4)\n",
    "cnn_model = CNNModel(output_size=4)\n",
    "vit_model = ViTModel(image_size=224, patch_size=16, dim=768, depth=6, heads=8, mlp_dim=1024, output_size=4)\n",
    "transformer_encoder_model = TransformerEncoderModel(image_size=224, dim=768, depth=6, heads=8, mlp_dim=1024, output_size=4)\n",
    "\n",
    "# Training each model\n",
    "#train_model(mlp_model, train_loader, val_loader, \"MLPModel\", num_epochs=30)\n",
    "train_model(cnn_model, train_loader, val_loader, \"CNNModel\", num_epochs=30)\n",
    "train_model(vit_model, train_loader, val_loader, \"ViTModel\", num_epochs=30)\n",
    "train_model(transformer_encoder_model, train_loader, val_loader, \"TransformerEncoderModel\", num_epochs=30)\n",
    "\n",
    "print(\"Training complete. Models are saved in ./regressors\")\n",
    "\n",
    "def visualize_predictions(model_name, dataset, index, prediction):\n",
    "    \"\"\"\n",
    "    Visualize ground truth and predicted bounding box for a single sample.\n",
    "    Args:\n",
    "        model_name: Name of the model being visualized.\n",
    "        dataset: Dataset object.\n",
    "        index: Index of the sample in the dataset.\n",
    "        prediction: Predicted bounding box.\n",
    "    \"\"\"\n",
    "    image, gt_bbox = dataset[index]\n",
    "\n",
    "    # Un-normalize image\n",
    "    image = unnormalize_image(image, mean, std)\n",
    "\n",
    "    # Convert tensor image to numpy for visualization\n",
    "    image_np = image.permute(1, 2, 0).numpy()\n",
    "    image_np = (image_np * 255).astype(\"uint8\")\n",
    "\n",
    "    # Plot the image\n",
    "    fig, ax = plt.subplots(1, figsize=(8, 8))\n",
    "    ax.imshow(image_np)\n",
    "\n",
    "    # Draw ground truth bounding box\n",
    "    gt_rect = patches.Rectangle(\n",
    "        (gt_bbox[0].item(), gt_bbox[1].item()),  # x_min, y_min\n",
    "        gt_bbox[2].item() - gt_bbox[0].item(),  # width\n",
    "        gt_bbox[3].item() - gt_bbox[1].item(),  # height\n",
    "        linewidth=2,\n",
    "        edgecolor=\"g\",\n",
    "        facecolor=\"none\",\n",
    "        label=\"Ground Truth\",\n",
    "    )\n",
    "    ax.add_patch(gt_rect)\n",
    "\n",
    "    # Draw predicted bounding box\n",
    "    pred_rect = patches.Rectangle(\n",
    "        (prediction[0].item(), prediction[1].item()),  # x_min, y_min\n",
    "        prediction[2].item() - prediction[0].item(),  # width\n",
    "        prediction[3].item() - prediction[1].item(),  # height\n",
    "        linewidth=2,\n",
    "        edgecolor=\"r\",\n",
    "        facecolor=\"none\",\n",
    "        label=\"Prediction\",\n",
    "    )\n",
    "    ax.add_patch(pred_rect)\n",
    "\n",
    "    ax.legend(loc=\"upper left\")\n",
    "    ax.set_title(f\"{model_name}: Ground Truth vs Prediction\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def test_model(model, test_loader, model_name, dataset):\n",
    "    \"\"\"Test the model and calculate metrics.\"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    maes, mses, r2_scores = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(test_loader, desc=f\"Testing {model_name}\"):\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            preds = model(images)\n",
    "\n",
    "            # Metrics\n",
    "            mae, mse, r2 = calculate_regression_metrics(preds, targets)\n",
    "            maes.append(mae)\n",
    "            mses.append(mse)\n",
    "            r2_scores.append(r2)\n",
    "\n",
    "    # Calculate mean metrics\n",
    "    mean_mae = sum(maes) / len(maes)\n",
    "    mean_mse = sum(mses) / len(mses)\n",
    "    mean_r2 = sum(r2_scores) / len(r2_scores)\n",
    "\n",
    "    print(f\"{model_name} Test Metrics:\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mean_mae:.4f}\")\n",
    "    print(f\"Mean Squared Error (MSE): {mean_mse:.4f}\")\n",
    "    print(f\"R2 Score: {mean_r2:.4f}\")\n",
    "\n",
    "    # Visualize one sample prediction\n",
    "    sample_index = 0  # Change this to visualize different samples\n",
    "    image, _ = dataset[sample_index]\n",
    "    image = image.unsqueeze(0).to(device)  # Add batch dimension\n",
    "    prediction = model(image).squeeze(0).cpu()  # Remove batch dimension\n",
    "    visualize_predictions(model_name, dataset, sample_index, prediction)\n",
    "\n",
    "# Load models\n",
    "#mlp_model = MLPModel(input_size=(3, 224, 224), output_size=4)\n",
    "cnn_model = CNNModel(output_size=4)\n",
    "vit_model = ViTModel(image_size=224, patch_size=16, dim=768, depth=6, heads=8, mlp_dim=1024, output_size=4)\n",
    "transformer_encoder_model = TransformerEncoderModel(image_size=224, dim=768, depth=6, heads=8, mlp_dim=1024, output_size=4)\n",
    "\n",
    "#mlp_model.load_state_dict(torch.load(\"./regressors/MLPModel.pth\"))\n",
    "cnn_model.load_state_dict(torch.load(\"./regressors/CNNModel.pth\"))\n",
    "vit_model.load_state_dict(torch.load(\"./regressors/ViTModel.pth\"))\n",
    "transformer_encoder_model.load_state_dict(torch.load(\"./regressors/TransformerEncoderModel.pth\"))\n",
    "\n",
    "# Test each model\n",
    "#test_model(mlp_model, test_loader, \"MLPModel\", test_dataset)\n",
    "test_model(cnn_model, test_loader, \"CNNModel\", test_dataset)\n",
    "test_model(vit_model, test_loader, \"ViTModel\", test_dataset)\n",
    "test_model(transformer_encoder_model, test_loader, \"TransformerEncoderModel\", test_dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
